{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d796ddc",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-26T12:38:47.702216Z",
     "iopub.status.busy": "2025-01-26T12:38:47.701884Z",
     "iopub.status.idle": "2025-01-26T12:38:48.994832Z",
     "shell.execute_reply": "2025-01-26T12:38:48.993793Z"
    },
    "papermill": {
     "duration": 1.300092,
     "end_time": "2025-01-26T12:38:48.997012",
     "exception": false,
     "start_time": "2025-01-26T12:38:47.696920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/isic-2024-package/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d7eecc",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-26T12:38:49.004927Z",
     "iopub.status.busy": "2025-01-26T12:38:49.004654Z",
     "iopub.status.idle": "2025-01-26T12:40:59.798311Z",
     "shell.execute_reply": "2025-01-26T12:40:59.797079Z"
    },
    "papermill": {
     "duration": 130.800053,
     "end_time": "2025-01-26T12:40:59.800612",
     "exception": false,
     "start_time": "2025-01-26T12:38:49.000559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./setup/efficientnet_pytorch-0.7.1.tar.gz\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet_pytorch==0.7.1) (2.4.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch==0.7.1) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch==0.7.1) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch==0.7.1) (1.13.2)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch==0.7.1) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch==0.7.1) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch==0.7.1) (2024.6.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet_pytorch==0.7.1) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet_pytorch==0.7.1) (1.3.0)\r\n",
      "Building wheels for collected packages: efficientnet_pytorch\r\n",
      "  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16427 sha256=7efda3bb67e969c0979bee073cf7319fc7a17a28c7ace801e1cd98860e2dfa24\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/05/b4/6c/b74f1eea0671c82226644bcbb0497d3e8cccd94febc031c466\r\n",
      "Successfully built efficientnet_pytorch\r\n",
      "Installing collected packages: efficientnet_pytorch\r\n",
      "Successfully installed efficientnet_pytorch-0.7.1\r\n",
      "Processing ./setup/munch-4.0.0-py2.py3-none-any.whl\r\n",
      "Installing collected packages: munch\r\n",
      "Successfully installed munch-4.0.0\r\n",
      "Processing ./setup/pretrainedmodels-0.7.4.tar.gz\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from pretrainedmodels==0.7.4) (2.4.0)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from pretrainedmodels==0.7.4) (0.19.0)\r\n",
      "Requirement already satisfied: munch in /opt/conda/lib/python3.10/site-packages (from pretrainedmodels==0.7.4) (4.0.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from pretrainedmodels==0.7.4) (4.66.4)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->pretrainedmodels==0.7.4) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->pretrainedmodels==0.7.4) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->pretrainedmodels==0.7.4) (1.13.2)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->pretrainedmodels==0.7.4) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->pretrainedmodels==0.7.4) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->pretrainedmodels==0.7.4) (2024.6.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->pretrainedmodels==0.7.4) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->pretrainedmodels==0.7.4) (9.5.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->pretrainedmodels==0.7.4) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->pretrainedmodels==0.7.4) (1.3.0)\r\n",
      "Building wheels for collected packages: pretrainedmodels\r\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=7819a565d4a536d3fb13cbe8c8839dba3ec82700997369bbd52977368a7d61b3\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/89/2c/e3/e433561365cb9cc3d3a221e0153f3f931fe7ebed27b2c4c4ca\r\n",
      "Successfully built pretrainedmodels\r\n",
      "Installing collected packages: pretrainedmodels\r\n",
      "Successfully installed pretrainedmodels-0.7.4\r\n"
     ]
    }
   ],
   "source": [
    "# скачиваем необходимые библиотеки и модели\n",
    "!mv ./setup/efficientnet_pytorch-0.7.1.tar.xyz ./setup/efficientnet_pytorch-0.7.1.tar.gz\n",
    "!mv ./setup/pretrainedmodels-0.7.4.tar.xyz ./setup/pretrainedmodels-0.7.4.tar.gz\n",
    "!pip install ./setup/efficientnet_pytorch-0.7.1.tar.gz\n",
    "!pip install ./setup/munch-4.0.0-py2.py3-none-any.whl\n",
    "!pip install ./setup/pretrainedmodels-0.7.4.tar.gz\n",
    "!rm -rf ./setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84de945b",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-26T12:40:59.810834Z",
     "iopub.status.busy": "2025-01-26T12:40:59.810539Z",
     "iopub.status.idle": "2025-01-26T12:41:45.027235Z",
     "shell.execute_reply": "2025-01-26T12:41:45.026575Z"
    },
    "papermill": {
     "duration": 45.224177,
     "end_time": "2025-01-26T12:41:45.029315",
     "exception": false,
     "start_time": "2025-01-26T12:40:59.805138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/albumentations/check_version.py:49: UserWarning: Error fetching version info <urlopen error [Errno -3] Temporary failure in name resolution>\n",
      "  data = fetch_version_info()\n"
     ]
    }
   ],
   "source": [
    "# импортируем необходимые библиотеки\n",
    "import os\n",
    "import cv2\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import gmean\n",
    "from typing import Optional, List\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import timm\n",
    "from timm.models.layers import BatchNormAct2d, SelectAdaptivePool2d\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.encoders import get_encoder\n",
    "from segmentation_models_pytorch.decoders.unet.decoder import UnetDecoder\n",
    "from segmentation_models_pytorch.decoders.unetplusplus.decoder import UnetPlusPlusDecoder\n",
    "from segmentation_models_pytorch.decoders.fpn.decoder import FPNDecoder\n",
    "from segmentation_models_pytorch.base import SegmentationHead\n",
    "from segmentation_models_pytorch.base import initialization as init\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef9d8573",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T12:41:45.040014Z",
     "iopub.status.busy": "2025-01-26T12:41:45.039508Z",
     "iopub.status.idle": "2025-01-26T12:41:45.133792Z",
     "shell.execute_reply": "2025-01-26T12:41:45.133089Z"
    },
    "papermill": {
     "duration": 0.101564,
     "end_time": "2025-01-26T12:41:45.135474",
     "exception": false,
     "start_time": "2025-01-26T12:41:45.033910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# рпределение модели для классификации с использованием предобученной модели из библиотеки timm\n",
    "\n",
    "class ISIC_Model(nn.Module):\n",
    "    def __init__(self, model_name, pretrained, num_classes):\n",
    "        super(ISIC_Model, self).__init__()\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes)\n",
    "\n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "#  модель Unet с предобученным энкодером EfficientNet и декодером для сегментации\n",
    "class IsicSMPEffnetUnetModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name = \"timm-efficientnet-b0\",\n",
    "        encoder_weights = 'noisy-student',\n",
    "        decoder_use_batchnorm: bool = True,\n",
    "        decoder_channels: List[int] = (256, 128, 64, 32, 16),\n",
    "        decoder_attention_type: Optional[str] = None,\n",
    "        in_features: int = 1280,\n",
    "        classes: int = 4\n",
    "    ):\n",
    "        super(IsicSMPEffnetUnetModel, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=3,\n",
    "            depth=5,\n",
    "            weights=encoder_weights,\n",
    "        )\n",
    "\n",
    "        self.conv_head = self.encoder.conv_head\n",
    "        self.bn2 = self.encoder.bn2\n",
    "        self.global_pool = self.encoder.global_pool\n",
    "\n",
    "        self.decoder = UnetDecoder(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            decoder_channels=decoder_channels,\n",
    "            n_blocks=5,\n",
    "            use_batchnorm=decoder_use_batchnorm,\n",
    "            center=True if encoder_name.startswith(\"vgg\") else False,\n",
    "            attention_type=decoder_attention_type,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=decoder_channels[-1],\n",
    "            out_channels=1,\n",
    "            activation=None,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "        self.fc = nn.Linear(in_features, 1024, bias=True)\n",
    "        self.cls_head = nn.Linear(1024, classes, bias=True)\n",
    "\n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.conv_head(x[-1])\n",
    "        x = self.bn2(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(-1, self.in_features)\n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.cls_head(x)\n",
    "        return x\n",
    "#  модель Unet++ с предобученным энкодером Res2Next и улучшенным декодером для сегментации\n",
    "\n",
    "class IsicSMPResnetUnetPlusPlusModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name = \"timm-res2next50\",\n",
    "        encoder_weights = 'imagenet',\n",
    "        decoder_use_batchnorm: bool = True,\n",
    "        decoder_channels: List[int] = (256, 128, 64, 32, 16),\n",
    "        decoder_attention_type: Optional[str] = None,\n",
    "        in_features: int = 2048,\n",
    "        classes: int = 4\n",
    "    ):\n",
    "        super(IsicSMPResnetUnetPlusPlusModel, self).__init__()\n",
    "        self.in_features = in_features\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=3,\n",
    "            depth=5,\n",
    "            weights=encoder_weights,\n",
    "        )\n",
    "\n",
    "        self.global_pool = SelectAdaptivePool2d()\n",
    "\n",
    "        self.decoder = UnetPlusPlusDecoder(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            decoder_channels=decoder_channels,\n",
    "            n_blocks=5,\n",
    "            use_batchnorm=decoder_use_batchnorm,\n",
    "            center=True if encoder_name.startswith(\"vgg\") else False,\n",
    "            attention_type=decoder_attention_type,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=decoder_channels[-1],\n",
    "            out_channels=1,\n",
    "            activation=None,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "        self.fc = nn.Linear(in_features, 1024, bias=True)\n",
    "        self.cls_head = nn.Linear(1024, classes, bias=True)\n",
    "\n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.global_pool(x[-1])\n",
    "        x = y_cls.view(-1, self.in_features)\n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.cls_head(x)\n",
    "        return x\n",
    "\n",
    "#  модель FPN с энкодером MIT и декодером Feature Pyramid Network для сегментации\n",
    "\n",
    "class IsicSMPMitFPNModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name = \"mit_b5\",\n",
    "        encoder_weights = 'imagenet',\n",
    "        classes: int = 4\n",
    "    ):\n",
    "        super(IsicSMPMitFPNModel, self).__init__()\n",
    "        self.in_features = 1280\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=3,\n",
    "            depth=5,\n",
    "            weights=encoder_weights,\n",
    "        )\n",
    "        \n",
    "        if encoder_name == \"mit_b5\":\n",
    "            self.conv_head = nn.Conv2d(512, self.in_features, 1, 1, bias=False)\n",
    "        elif encoder_name == \"mit_b0\":\n",
    "            self.conv_head = nn.Conv2d(256, self.in_features, 1, 1, bias=False)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        self.bn2 = BatchNormAct2d(num_features=self.in_features)\n",
    "        self.global_pool = SelectAdaptivePool2d()\n",
    "\n",
    "        self.decoder = FPNDecoder(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            encoder_depth=5,\n",
    "            pyramid_channels=256,\n",
    "            segmentation_channels=128,\n",
    "            dropout=0.2,\n",
    "            merge_policy=\"add\",\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=self.decoder.out_channels,\n",
    "            out_channels=1,\n",
    "            kernel_size=1,\n",
    "            upsampling=4,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.in_features, 1024, bias=True)\n",
    "        self.cls_head = nn.Linear(1024, classes, bias=True)\n",
    "\n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        x = self.conv_head(x[-1])\n",
    "        x = self.bn2(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(-1, self.in_features)\n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.cls_head(x)\n",
    "        return x\n",
    "# Класс для объединения моделей классификации и сегментации с использованием различных энкодеров\n",
    "\n",
    "class ISIC_Cls_Seg_Aux_Model(nn.Module):\n",
    "    def __init__(self, encoder_name=\"timm-efficientnet-b0\", encoder_weights = 'noisy-student', in_features=1280, classes=10, img_size=256):\n",
    "        super(ISIC_Cls_Seg_Aux_Model, self).__init__()\n",
    "        if \"timm-efficientnet\" in encoder_name:\n",
    "            self.model = IsicSMPEffnetUnetModel(\n",
    "                encoder_name=encoder_name,\n",
    "                encoder_weights=encoder_weights,\n",
    "                in_features=in_features,\n",
    "                classes=classes)\n",
    "        elif \"timm-res2next50\" in encoder_name:\n",
    "            self.model = IsicSMPResnetUnetPlusPlusModel(\n",
    "                encoder_name=encoder_name,\n",
    "                encoder_weights=encoder_weights,\n",
    "                in_features=in_features,\n",
    "                classes=classes)\n",
    "        elif \"mit_b\" in encoder_name:\n",
    "            self.model = IsicSMPMitFPNModel(\n",
    "                encoder_name=encoder_name,\n",
    "                encoder_weights=encoder_weights,\n",
    "                classes=classes)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "# класс датасета для загрузки изображений ISIC из HDF5 файла с применением различных трансформаций\n",
    "class ISIC_Dataset(Dataset):\n",
    "    # инициализация HDF5 файла и определение трансформаций для изображений разных размеров\n",
    "    def __init__(self, df, file_hdf):\n",
    "        self.fp_hdf = h5py.File(file_hdf, mode=\"r\")\n",
    "        self.isic_ids = df['isic_id'].values\n",
    "        self.transform_64 = albu.Compose([\n",
    "            albu.Resize(64, 64),\n",
    "            albu.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        self.transform_128 = albu.Compose([\n",
    "            albu.Resize(128, 128),\n",
    "            albu.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        self.transform_224 = albu.Compose([\n",
    "            albu.Resize(224, 224),\n",
    "            albu.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        self.transform_256 = albu.Compose([\n",
    "            albu.Resize(256, 256),\n",
    "            albu.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        self.transform_288 = albu.Compose([\n",
    "            albu.Resize(288, 288),\n",
    "            albu.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        self.transform_384 = albu.Compose([\n",
    "            albu.Resize(384, 384),\n",
    "            albu.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    # возвращает количество изображений в датасете\n",
    "    def __len__(self):\n",
    "        return len(self.isic_ids)\n",
    "    # загружает изображение по индексу, применяет трансформации и возвращает их в нескольких размерах\n",
    "    def __getitem__(self, index):\n",
    "        isic_id = self.isic_ids[index]\n",
    "        image = np.array(Image.open(BytesIO(self.fp_hdf[isic_id][()])))\n",
    "        image_64 = self.transform_64(image=image)['image']\n",
    "        image_128 = self.transform_128(image=image)['image']\n",
    "        image_224 = self.transform_224(image=image)['image']\n",
    "        image_256 = self.transform_256(image=image)['image']\n",
    "        image_288 = self.transform_288(image=image)['image']\n",
    "        image_384 = self.transform_384(image=image)['image']\n",
    "        return isic_id, image_64, image_128, image_224, image_256, image_288, image_384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "317dc668",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T12:41:45.145146Z",
     "iopub.status.busy": "2025-01-26T12:41:45.144903Z",
     "iopub.status.idle": "2025-01-26T12:41:45.163750Z",
     "shell.execute_reply": "2025-01-26T12:41:45.162931Z"
    },
    "papermill": {
     "duration": 0.025441,
     "end_time": "2025-01-26T12:41:45.165366",
     "exception": false,
     "start_time": "2025-01-26T12:41:45.139925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv(\"/kaggle/input/isic-2024-challenge/sample_submission.csv\")\n",
    "hdf5_test_file = '/kaggle/input/isic-2024-challenge/test-image.hdf5'\n",
    "classes_2024 = ['target', 'MEL','BCC','SCC','NV']\n",
    "USE_IMAGE_FEAT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfd29cd1",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-26T12:41:45.175124Z",
     "iopub.status.busy": "2025-01-26T12:41:45.174871Z",
     "iopub.status.idle": "2025-01-26T12:42:10.561944Z",
     "shell.execute_reply": "2025-01-26T12:42:10.560783Z"
    },
    "papermill": {
     "duration": 25.393944,
     "end_time": "2025-01-26T12:42:10.563646",
     "exception": false,
     "start_time": "2025-01-26T12:41:45.169702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n"
     ]
    }
   ],
   "source": [
    "if USE_IMAGE_FEAT:\n",
    "    vit_tiny_384_model = ISIC_Model(model_name='vit_tiny_patch16_384.augreg_in21k_ft_in1k', pretrained=False, num_classes=len(classes_2024))\n",
    "    vit_tiny_384_model.load_state_dict(torch.load('/kaggle/input/isic-2024-img-ckpts/img_ckpt_noval_exp3/vit_tiny_patch16_384.augreg_in21k_ft_in1k_384_epoch9_ema.pt'))\n",
    "    vit_tiny_384_model.cuda()\n",
    "    vit_tiny_384_model.eval()\n",
    "\n",
    "    swin_tiny_256_model = ISIC_Model(model_name='swinv2_tiny_window8_256.ms_in1k', pretrained=False, num_classes=len(classes_2024))\n",
    "    swin_tiny_256_model.load_state_dict(torch.load('/kaggle/input/isic-2024-img-ckpts/img_ckpt_noval_exp3/swinv2_tiny_window8_256.ms_in1k_256_epoch7_ema.pt'))\n",
    "    swin_tiny_256_model.cuda()\n",
    "    swin_tiny_256_model.eval()\n",
    "\n",
    "    convnextv2_tiny_288_model = ISIC_Model(model_name='convnextv2_tiny.fcmae_ft_in22k_in1k', pretrained=False, num_classes=len(classes_2024))\n",
    "    convnextv2_tiny_288_model.load_state_dict(torch.load('/kaggle/input/isic-2024-img-ckpts/img_ckpt_noval_exp3/convnextv2_tiny.fcmae_ft_in22k_in1k_288_epoch8_ema.pt'))\n",
    "    convnextv2_tiny_288_model.cuda()\n",
    "    convnextv2_tiny_288_model.eval()\n",
    "\n",
    "    swin_tiny_224_model = ISIC_Model(model_name='swin_tiny_patch4_window7_224.ms_in1k', pretrained=False, num_classes=len(classes_2024))\n",
    "    swin_tiny_224_model.load_state_dict(torch.load('/kaggle/input/isic-2024-img-ckpts/img_ckpt_noval_exp0/swin_tiny_patch4_window7_224.ms_in1k_224_epoch8_ema.pt'))\n",
    "    swin_tiny_224_model.cuda()\n",
    "    swin_tiny_224_model.eval()\n",
    "\n",
    "    convnextv2_base_128_model = ISIC_Model(model_name='convnextv2_base.fcmae_ft_in22k_in1k', pretrained=False, num_classes=len(classes_2024))\n",
    "    convnextv2_base_128_model.load_state_dict(torch.load('/kaggle/input/isic-2024-img-ckpts/img_ckpt_noval_exp0/convnextv2_base.fcmae_ft_in22k_in1k_128_epoch8_ema.pt'))\n",
    "    convnextv2_base_128_model.cuda()\n",
    "    convnextv2_base_128_model.eval()\n",
    "\n",
    "    convnextv2_large_64_model = ISIC_Model(model_name='convnextv2_large.fcmae_ft_in22k_in1k', pretrained=False, num_classes=len(classes_2024))\n",
    "    convnextv2_large_64_model.load_state_dict(torch.load('/kaggle/input/isic-2024-img-ckpts/img_ckpt_noval_exp0/convnextv2_large.fcmae_ft_in22k_in1k_64_epoch8_ema.pt'))\n",
    "    convnextv2_large_64_model.cuda()\n",
    "    convnextv2_large_64_model.eval()\n",
    "\n",
    "    coatnet_224_model = ISIC_Model(model_name='coatnet_rmlp_1_rw_224.sw_in1k', pretrained=False, num_classes=len(classes_2024))\n",
    "    coatnet_224_model.load_state_dict(torch.load('/kaggle/input/isic-2024-img-ckpts/img_ckpt_noval_exp0/coatnet_rmlp_1_rw_224.sw_in1k_224_epoch8_ema.pt'))\n",
    "    coatnet_224_model.cuda()\n",
    "    coatnet_224_model.eval()\n",
    "\n",
    "    eb3_aux_224_model = ISIC_Cls_Seg_Aux_Model(encoder_name=\"timm-efficientnet-b3\", encoder_weights=None, in_features=1536, classes=len(classes_2024))\n",
    "    eb3_aux_224_model.load_state_dict(torch.load('/kaggle/input/isic-2024-img-ckpts/img_ckpt_noval_aux_exp1/timm-efficientnet-b3_224_epoch8_aux_ema.pt'))\n",
    "    eb3_aux_224_model.cuda()\n",
    "    eb3_aux_224_model.eval()\n",
    "\n",
    "    mit_b0_aux_384_model = ISIC_Cls_Seg_Aux_Model(encoder_name=\"mit_b0\", encoder_weights=None, in_features=1280, classes=len(classes_2024))\n",
    "    mit_b0_aux_384_model.load_state_dict(torch.load('/kaggle/input/isic-2024-img-ckpts/img_ckpt_noval_aux_exp1/mit_b0_384_epoch8_aux_ema.pt'))\n",
    "    mit_b0_aux_384_model.cuda()\n",
    "    mit_b0_aux_384_model.eval()\n",
    "\n",
    "    mit_b5_aux_224_model = ISIC_Cls_Seg_Aux_Model(encoder_name=\"mit_b5\", encoder_weights=None, in_features=1280, classes=len(classes_2024))\n",
    "    mit_b5_aux_224_model.load_state_dict(torch.load('/kaggle/input/isic-2024-img-ckpts/img_ckpt_noval_aux_exp1/mit_b5_224_epoch8_aux_ema.pt'))\n",
    "    mit_b5_aux_224_model.cuda()\n",
    "    mit_b5_aux_224_model.eval()\n",
    "\n",
    "    test_dataset = ISIC_Dataset(df=df_sub, file_hdf=hdf5_test_file)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, num_workers=2, shuffle=False)\n",
    "    \n",
    "    test_isic_ids = []\n",
    "    img_feats = []\n",
    "    for batch_isic_id, batch_image_64, batch_image_128, batch_image_224, batch_image_256, batch_image_288, batch_image_384 in tqdm(test_loader):\n",
    "        test_isic_ids.extend(batch_isic_id)\n",
    "        batch_image_64 = batch_image_64.cuda()\n",
    "        batch_image_128 = batch_image_128.cuda()\n",
    "        batch_image_224 = batch_image_224.cuda()\n",
    "        batch_image_256 = batch_image_256.cuda()\n",
    "        batch_image_288 = batch_image_288.cuda()\n",
    "        batch_image_384 = batch_image_384.cuda()\n",
    "\n",
    "        batch_feat = []\n",
    "        with torch.cuda.amp.autocast(), torch.no_grad():\n",
    "            vit_tiny_384_pred = torch.sigmoid(vit_tiny_384_model(batch_image_384))\n",
    "            vit_tiny_384_pred = vit_tiny_384_pred.data.cpu().numpy()[:,0]\n",
    "            batch_feat.append(vit_tiny_384_pred)\n",
    "\n",
    "            swin_tiny_256_pred = torch.sigmoid(swin_tiny_256_model(batch_image_256))\n",
    "            swin_tiny_256_pred = swin_tiny_256_pred.data.cpu().numpy()[:,0]\n",
    "            batch_feat.append(swin_tiny_256_pred)\n",
    "\n",
    "            convnextv2_tiny_pred = torch.sigmoid(convnextv2_tiny_288_model(batch_image_288))\n",
    "            convnextv2_tiny_pred = convnextv2_tiny_pred.data.cpu().numpy()[:,0]\n",
    "            batch_feat.append(convnextv2_tiny_pred)\n",
    "\n",
    "            swin_tiny_224_pred = torch.sigmoid(swin_tiny_224_model(batch_image_224))\n",
    "            swin_tiny_224_pred = swin_tiny_224_pred.data.cpu().numpy()[:,0]\n",
    "            batch_feat.append(swin_tiny_224_pred)\n",
    "\n",
    "            convnextv2_base_128_pred = torch.sigmoid(convnextv2_base_128_model(batch_image_128))\n",
    "            convnextv2_base_128_pred = convnextv2_base_128_pred.data.cpu().numpy()[:,0]\n",
    "            batch_feat.append(convnextv2_base_128_pred)\n",
    "\n",
    "            convnextv2_large_64_pred = torch.sigmoid(convnextv2_large_64_model(batch_image_64))\n",
    "            convnextv2_large_64_pred = convnextv2_large_64_pred.data.cpu().numpy()[:,0]\n",
    "            batch_feat.append(convnextv2_large_64_pred)\n",
    "\n",
    "            coatnet_224_pred = torch.sigmoid(coatnet_224_model(batch_image_224))\n",
    "            coatnet_224_pred = coatnet_224_pred.data.cpu().numpy()[:,0]\n",
    "            batch_feat.append(coatnet_224_pred)\n",
    "\n",
    "            eb3_aux_224_pred = torch.sigmoid(eb3_aux_224_model(batch_image_224))\n",
    "            eb3_aux_224_pred = eb3_aux_224_pred.data.cpu().numpy()[:,0]\n",
    "            batch_feat.append(eb3_aux_224_pred)\n",
    "\n",
    "            mit_b5_aux_224_pred = torch.sigmoid(mit_b5_aux_224_model(batch_image_224))\n",
    "            mit_b5_aux_224_pred = mit_b5_aux_224_pred.data.cpu().numpy()[:,0]\n",
    "            batch_feat.append(mit_b5_aux_224_pred)\n",
    "\n",
    "            mit_b0_aux_384_pred = torch.sigmoid(mit_b0_aux_384_model(batch_image_384))\n",
    "            mit_b0_aux_384_pred = mit_b0_aux_384_pred.data.cpu().numpy()[:,0]\n",
    "            batch_feat.append(mit_b0_aux_384_pred)\n",
    "\n",
    "        batch_feat = np.stack(batch_feat, -1)\n",
    "        img_feats.append(batch_feat)\n",
    "    test_isic_ids = np.array(test_isic_ids)\n",
    "    img_feats = np.concatenate(img_feats, axis=0)\n",
    "    img_feats_dict = dict(zip(test_isic_ids, img_feats))\n",
    "\n",
    "    del vit_tiny_384_model\n",
    "    del swin_tiny_256_model\n",
    "    del convnextv2_tiny_288_model\n",
    "    del swin_tiny_224_model\n",
    "    del convnextv2_base_128_model\n",
    "    del convnextv2_large_64_model\n",
    "    del coatnet_224_model\n",
    "    del eb3_aux_224_model\n",
    "    del mit_b0_aux_384_model\n",
    "    del mit_b5_aux_224_model\n",
    "\n",
    "    del test_isic_ids\n",
    "    del img_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "032c5411",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-26T12:42:10.574643Z",
     "iopub.status.busy": "2025-01-26T12:42:10.574327Z",
     "iopub.status.idle": "2025-01-26T12:42:10.600213Z",
     "shell.execute_reply": "2025-01-26T12:42:10.599429Z"
    },
    "papermill": {
     "duration": 0.033527,
     "end_time": "2025-01-26T12:42:10.601820",
     "exception": false,
     "start_time": "2025-01-26T12:42:10.568293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "id_col = 'isic_id'\n",
    "target_col = 'target'\n",
    "group_col = 'patient_id'\n",
    "\n",
    "# имеющиеся \"с коробки\" фичи и их описание\n",
    "num_cols = [\n",
    "    'age_approx',                        # Approximate age of patient at time of imaging.\n",
    "    'clin_size_long_diam_mm',            # Maximum diameter of the lesion (mm).+\n",
    "    'tbp_lv_A',                          # A inside  lesion.+\n",
    "    'tbp_lv_Aext',                       # A outside lesion.+\n",
    "    'tbp_lv_B',                          # B inside  lesion.+\n",
    "    'tbp_lv_Bext',                       # B outside lesion.+ \n",
    "    'tbp_lv_C',                          # Chroma inside  lesion.+\n",
    "    'tbp_lv_Cext',                       # Chroma outside lesion.+\n",
    "    'tbp_lv_H',                          # Hue inside the lesion; calculated as the angle of A* and B* in LAB* color space. Typical values range from 25 (red) to 75 (brown).+\n",
    "    'tbp_lv_Hext',                       # Hue outside lesion.+\n",
    "    'tbp_lv_L',                          # L inside lesion.+\n",
    "    'tbp_lv_Lext',                       # L outside lesion.+\n",
    "    'tbp_lv_areaMM2',                    # Area of lesion (mm^2).+\n",
    "    'tbp_lv_area_perim_ratio',           # Border jaggedness, the ratio between lesions perimeter and area. Circular lesions will have low values; irregular shaped lesions will have higher values. Values range 0-10.+\n",
    "    'tbp_lv_color_std_mean',             # Color irregularity, calculated as the variance of colors within the lesion's boundary.\n",
    "    'tbp_lv_deltaA',                     # Average A contrast (inside vs. outside lesion).+\n",
    "    'tbp_lv_deltaB',                     # Average B contrast (inside vs. outside lesion).+\n",
    "    'tbp_lv_deltaL',                     # Average L contrast (inside vs. outside lesion).+\n",
    "    'tbp_lv_deltaLB',                    #\n",
    "    'tbp_lv_deltaLBnorm',                # Contrast between the lesion and its immediate surrounding skin. Low contrast lesions tend to be faintly visible such as freckles; high contrast lesions tend to be those with darker pigment. Calculated as the average delta LB of the lesion relative to its immediate background in LAB* color space. Typical values range from 5.5 to 25.+\n",
    "    'tbp_lv_eccentricity',               # Eccentricity.+\n",
    "    'tbp_lv_minorAxisMM',                # Smallest lesion diameter (mm).+\n",
    "    'tbp_lv_nevi_confidence',            # Nevus confidence score (0-100 scale) is a convolutional neural network classifier estimated probability that the lesion is a nevus. The neural network was trained on approximately 57,000 lesions that were classified and labeled by a dermatologist.+,++\n",
    "    'tbp_lv_norm_border',                # Border irregularity (0-10 scale); the normalized average of border jaggedness and asymmetry.+\n",
    "    'tbp_lv_norm_color',                 # Color variation (0-10 scale); the normalized average of color asymmetry and color irregularity.+\n",
    "    'tbp_lv_perimeterMM',                # Perimeter of lesion (mm).+\n",
    "    'tbp_lv_radial_color_std_max',       # Color asymmetry, a measure of asymmetry of the spatial distribution of color within the lesion. This score is calculated by looking at the average standard deviation in LAB* color space within concentric rings originating from the lesion center. Values range 0-10.+\n",
    "    'tbp_lv_stdL',                       # Standard deviation of L inside  lesion.+\n",
    "    'tbp_lv_stdLExt',                    # Standard deviation of L outside lesion.+\n",
    "    'tbp_lv_symm_2axis',                 # Border asymmetry; a measure of asymmetry of the lesion's contour about an axis perpendicular to the lesion's most symmetric axis. Lesions with two axes of symmetry will therefore have low scores (more symmetric), while lesions with only one or zero axes of symmetry will have higher scores (less symmetric). This score is calculated by comparing opposite halves of the lesion contour over many degrees of rotation. The angle where the halves are most similar identifies the principal axis of symmetry, while the second axis of symmetry is perpendicular to the principal axis. Border asymmetry is reported as the asymmetry value about this second axis. Values range 0-10.+\n",
    "    'tbp_lv_symm_2axis_angle',           # Lesion border asymmetry angle.+\n",
    "    'tbp_lv_x',                          # X-coordinate of the lesion on 3D TBP.+\n",
    "    'tbp_lv_y',                          # Y-coordinate of the lesion on 3D TBP.+\n",
    "    'tbp_lv_z',                          # Z-coordinate of the lesion on 3D TBP.+\n",
    "]\n",
    "\n",
    "# feature engineering на основе тех фич\n",
    "new_num_cols = [\n",
    "    'lesion_size_ratio',             # tbp_lv_minorAxisMM      / clin_size_long_diam_mm\n",
    "    'lesion_shape_index',            # tbp_lv_areaMM2          / tbp_lv_perimeterMM **2\n",
    "    'hue_contrast',                  # tbp_lv_H                - tbp_lv_Hext              abs\n",
    "    'luminance_contrast',            # tbp_lv_L                - tbp_lv_Lext              abs\n",
    "    'lesion_color_difference',       # tbp_lv_deltaA **2       + tbp_lv_deltaB **2 + tbp_lv_deltaL **2  sqrt  \n",
    "    'border_complexity',             # tbp_lv_norm_border      + tbp_lv_symm_2axis\n",
    "    'color_uniformity',              # tbp_lv_color_std_mean   / tbp_lv_radial_color_std_max\n",
    "\n",
    "    'position_distance_3d',          # tbp_lv_x **2 + tbp_lv_y **2 + tbp_lv_z **2  sqrt\n",
    "    'perimeter_to_area_ratio',       # tbp_lv_perimeterMM      / tbp_lv_areaMM2\n",
    "    'area_to_perimeter_ratio',       # tbp_lv_areaMM2          / tbp_lv_perimeterMM\n",
    "    'lesion_visibility_score',       # tbp_lv_deltaLBnorm      + tbp_lv_norm_color\n",
    "    'symmetry_border_consistency',   # tbp_lv_symm_2axis       * tbp_lv_norm_border\n",
    "    'consistency_symmetry_border',   # tbp_lv_symm_2axis       * tbp_lv_norm_border / (tbp_lv_symm_2axis + tbp_lv_norm_border)\n",
    "\n",
    "    'color_consistency',             # tbp_lv_stdL             / tbp_lv_Lext\n",
    "    'consistency_color',             # tbp_lv_stdL*tbp_lv_Lext / tbp_lv_stdL + tbp_lv_Lext\n",
    "    'size_age_interaction',          # clin_size_long_diam_mm  * age_approx\n",
    "    'hue_color_std_interaction',     # tbp_lv_H                * tbp_lv_color_std_mean\n",
    "    'lesion_severity_index',         # tbp_lv_norm_border      + tbp_lv_norm_color + tbp_lv_eccentricity / 3\n",
    "    'shape_complexity_index',        # border_complexity       + lesion_shape_index\n",
    "    'color_contrast_index',          # tbp_lv_deltaA + tbp_lv_deltaB + tbp_lv_deltaL + tbp_lv_deltaLBnorm\n",
    "\n",
    "    'log_lesion_area',               # tbp_lv_areaMM2          + 1  np.log\n",
    "    'normalized_lesion_size',        # clin_size_long_diam_mm  / age_approx\n",
    "    'mean_hue_difference',           # tbp_lv_H                + tbp_lv_Hext    / 2\n",
    "    'std_dev_contrast',              # tbp_lv_deltaA **2 + tbp_lv_deltaB **2 + tbp_lv_deltaL **2   / 3  np.sqrt\n",
    "    'color_shape_composite_index',   # tbp_lv_color_std_mean   + bp_lv_area_perim_ratio + tbp_lv_symm_2axis   / 3\n",
    "    'lesion_orientation_3d',         # tbp_lv_y                , tbp_lv_x  np.arctan2\n",
    "    'overall_color_difference',      # tbp_lv_deltaA           + tbp_lv_deltaB + tbp_lv_deltaL   / 3\n",
    "\n",
    "    'symmetry_perimeter_interaction',# tbp_lv_symm_2axis       * tbp_lv_perimeterMM\n",
    "    'comprehensive_lesion_index',    # tbp_lv_area_perim_ratio + tbp_lv_eccentricity + bp_lv_norm_color + tbp_lv_symm_2axis   / 4\n",
    "    'color_variance_ratio',          # tbp_lv_color_std_mean   / tbp_lv_stdLExt\n",
    "    'border_color_interaction',      # tbp_lv_norm_border      * tbp_lv_norm_color\n",
    "    'border_color_interaction_2',\n",
    "    'size_color_contrast_ratio',     # clin_size_long_diam_mm  / tbp_lv_deltaLBnorm\n",
    "    'age_normalized_nevi_confidence',# tbp_lv_nevi_confidence  / age_approx\n",
    "    'age_normalized_nevi_confidence_2',\n",
    "    'color_asymmetry_index',         # tbp_lv_symm_2axis       * tbp_lv_radial_color_std_max\n",
    "\n",
    "    'volume_approximation_3d',       # tbp_lv_areaMM2          * sqrt(tbp_lv_x**2 + tbp_lv_y**2 + tbp_lv_z**2)\n",
    "    'color_range',                   # abs(tbp_lv_L - tbp_lv_Lext) + abs(tbp_lv_A - tbp_lv_Aext) + abs(tbp_lv_B - tbp_lv_Bext)\n",
    "    'shape_color_consistency',       # tbp_lv_eccentricity     * tbp_lv_color_std_mean\n",
    "    'border_length_ratio',           # tbp_lv_perimeterMM      / pi * sqrt(tbp_lv_areaMM2 / pi)\n",
    "    'age_size_symmetry_index',       # age_approx              * clin_size_long_diam_mm * tbp_lv_symm_2axis\n",
    "    'index_age_size_symmetry',       # age_approx              * tbp_lv_areaMM2 * tbp_lv_symm_2axis\n",
    "]\n",
    "\n",
    "cat_cols = ['sex', 'anatom_site_general', 'tbp_tile_type', 'tbp_lv_location', 'tbp_lv_location_simple', 'attribution']\n",
    "norm_cols = [f'{col}_patient_norm' for col in num_cols + new_num_cols]\n",
    "special_cols = ['count_per_patient']\n",
    "feature_cols = num_cols + new_num_cols + cat_cols + norm_cols + special_cols\n",
    "\n",
    "# FE-функция\n",
    "def read_data(path, err = 1e-5):\n",
    "    return (\n",
    "        pl.read_csv(path)\n",
    "        .with_columns(\n",
    "            pl.col('age_approx').cast(pl.String).replace('NA', np.nan).cast(pl.Float64),\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col(pl.Float64).fill_nan(pl.col(pl.Float64).median()), # You may want to impute test data with train\n",
    "        )\n",
    "        .with_columns(\n",
    "            lesion_size_ratio              = pl.col('tbp_lv_minorAxisMM') / pl.col('clin_size_long_diam_mm'),\n",
    "            lesion_shape_index             = pl.col('tbp_lv_areaMM2') / (pl.col('tbp_lv_perimeterMM') ** 2),\n",
    "            hue_contrast                   = (pl.col('tbp_lv_H') - pl.col('tbp_lv_Hext')).abs(),\n",
    "            luminance_contrast             = (pl.col('tbp_lv_L') - pl.col('tbp_lv_Lext')).abs(),\n",
    "            lesion_color_difference        = (pl.col('tbp_lv_deltaA') ** 2 + pl.col('tbp_lv_deltaB') ** 2 + pl.col('tbp_lv_deltaL') ** 2).sqrt(),\n",
    "            border_complexity              = pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_symm_2axis'),\n",
    "            color_uniformity               = pl.col('tbp_lv_color_std_mean') / (pl.col('tbp_lv_radial_color_std_max') + err),\n",
    "        )\n",
    "        .with_columns(\n",
    "            position_distance_3d           = (pl.col('tbp_lv_x') ** 2 + pl.col('tbp_lv_y') ** 2 + pl.col('tbp_lv_z') ** 2).sqrt(),\n",
    "            perimeter_to_area_ratio        = pl.col('tbp_lv_perimeterMM') / pl.col('tbp_lv_areaMM2'),\n",
    "            area_to_perimeter_ratio        = pl.col('tbp_lv_areaMM2') / pl.col('tbp_lv_perimeterMM'),\n",
    "            lesion_visibility_score        = pl.col('tbp_lv_deltaLBnorm') + pl.col('tbp_lv_norm_color'),\n",
    "            combined_anatomical_site       = pl.col('anatom_site_general') + '_' + pl.col('tbp_lv_location'),\n",
    "            symmetry_border_consistency    = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_norm_border'),\n",
    "            consistency_symmetry_border    = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_norm_border') / (pl.col('tbp_lv_symm_2axis') + pl.col('tbp_lv_norm_border')),\n",
    "        )\n",
    "        .with_columns(\n",
    "            color_consistency              = pl.col('tbp_lv_stdL') / pl.col('tbp_lv_Lext'),\n",
    "            consistency_color              = pl.col('tbp_lv_stdL') * pl.col('tbp_lv_Lext') / (pl.col('tbp_lv_stdL') + pl.col('tbp_lv_Lext')),\n",
    "            size_age_interaction           = pl.col('clin_size_long_diam_mm') * pl.col('age_approx'),\n",
    "            hue_color_std_interaction      = pl.col('tbp_lv_H') * pl.col('tbp_lv_color_std_mean'),\n",
    "            lesion_severity_index          = (pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_norm_color') + pl.col('tbp_lv_eccentricity')) / 3,\n",
    "            shape_complexity_index         = pl.col('border_complexity') + pl.col('lesion_shape_index'),\n",
    "            color_contrast_index           = pl.col('tbp_lv_deltaA') + pl.col('tbp_lv_deltaB') + pl.col('tbp_lv_deltaL') + pl.col('tbp_lv_deltaLBnorm'),\n",
    "        )\n",
    "        .with_columns(\n",
    "            log_lesion_area                = (pl.col('tbp_lv_areaMM2') + 1).log(),\n",
    "            normalized_lesion_size         = pl.col('clin_size_long_diam_mm') / pl.col('age_approx'),\n",
    "            mean_hue_difference            = (pl.col('tbp_lv_H') + pl.col('tbp_lv_Hext')) / 2,\n",
    "            std_dev_contrast               = ((pl.col('tbp_lv_deltaA') ** 2 + pl.col('tbp_lv_deltaB') ** 2 + pl.col('tbp_lv_deltaL') ** 2) / 3).sqrt(),\n",
    "            color_shape_composite_index    = (pl.col('tbp_lv_color_std_mean') + pl.col('tbp_lv_area_perim_ratio') + pl.col('tbp_lv_symm_2axis')) / 3,\n",
    "            lesion_orientation_3d          = pl.arctan2(pl.col('tbp_lv_y'), pl.col('tbp_lv_x')),\n",
    "            overall_color_difference       = (pl.col('tbp_lv_deltaA') + pl.col('tbp_lv_deltaB') + pl.col('tbp_lv_deltaL')) / 3,\n",
    "        )\n",
    "        .with_columns(\n",
    "            symmetry_perimeter_interaction = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_perimeterMM'),\n",
    "            comprehensive_lesion_index     = (pl.col('tbp_lv_area_perim_ratio') + pl.col('tbp_lv_eccentricity') + pl.col('tbp_lv_norm_color') + pl.col('tbp_lv_symm_2axis')) / 4,\n",
    "            color_variance_ratio           = pl.col('tbp_lv_color_std_mean') / pl.col('tbp_lv_stdLExt'),\n",
    "            border_color_interaction       = pl.col('tbp_lv_norm_border') * pl.col('tbp_lv_norm_color'),\n",
    "            border_color_interaction_2     = pl.col('tbp_lv_norm_border') * pl.col('tbp_lv_norm_color') / (pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_norm_color')),\n",
    "            size_color_contrast_ratio      = pl.col('clin_size_long_diam_mm') / pl.col('tbp_lv_deltaLBnorm'),\n",
    "            age_normalized_nevi_confidence = pl.col('tbp_lv_nevi_confidence') / pl.col('age_approx'),\n",
    "            age_normalized_nevi_confidence_2 = (pl.col('clin_size_long_diam_mm')**2 + pl.col('age_approx')**2).sqrt(),\n",
    "            color_asymmetry_index          = pl.col('tbp_lv_radial_color_std_max') * pl.col('tbp_lv_symm_2axis'),\n",
    "        )\n",
    "        .with_columns(\n",
    "            volume_approximation_3d        = pl.col('tbp_lv_areaMM2') * (pl.col('tbp_lv_x')**2 + pl.col('tbp_lv_y')**2 + pl.col('tbp_lv_z')**2).sqrt(),\n",
    "            color_range                    = (pl.col('tbp_lv_L') - pl.col('tbp_lv_Lext')).abs() + (pl.col('tbp_lv_A') - pl.col('tbp_lv_Aext')).abs() + (pl.col('tbp_lv_B') - pl.col('tbp_lv_Bext')).abs(),\n",
    "            shape_color_consistency        = pl.col('tbp_lv_eccentricity') * pl.col('tbp_lv_color_std_mean'),\n",
    "            border_length_ratio            = pl.col('tbp_lv_perimeterMM') / (2 * np.pi * (pl.col('tbp_lv_areaMM2') / np.pi).sqrt()),\n",
    "            age_size_symmetry_index        = pl.col('age_approx') * pl.col('clin_size_long_diam_mm') * pl.col('tbp_lv_symm_2axis'),\n",
    "            index_age_size_symmetry        = pl.col('age_approx') * pl.col('tbp_lv_areaMM2') * pl.col('tbp_lv_symm_2axis'),\n",
    "        )\n",
    "        .with_columns(\n",
    "            ((pl.col(col) - pl.col(col).mean().over('patient_id')) / (pl.col(col).std().over('patient_id') + err)).alias(f'{col}_patient_norm') for col in (num_cols + new_num_cols)\n",
    "        )\n",
    "        .with_columns(\n",
    "            count_per_patient = pl.col('isic_id').count().over('patient_id'),\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col(cat_cols).cast(pl.Categorical),\n",
    "        )\n",
    "        .to_pandas()\n",
    "        .set_index(id_col)\n",
    "    )\n",
    "\n",
    "#onehotencoding функция\n",
    "def preprocess(df_train, df_test):\n",
    "    global cat_cols\n",
    "    \n",
    "    encoder = OneHotEncoder(sparse_output=False, dtype=np.int32, handle_unknown='ignore')\n",
    "    encoder.fit(df_train[cat_cols])\n",
    "    \n",
    "    new_cat_cols = [f'onehot_{i}' for i in range(len(encoder.get_feature_names_out()))]\n",
    "\n",
    "    df_train[new_cat_cols] = encoder.transform(df_train[cat_cols])\n",
    "    df_train[new_cat_cols] = df_train[new_cat_cols].astype('category')\n",
    "\n",
    "    df_test[new_cat_cols] = encoder.transform(df_test[cat_cols])\n",
    "    df_test[new_cat_cols] = df_test[new_cat_cols].astype('category')\n",
    "\n",
    "    for col in cat_cols:\n",
    "        feature_cols.remove(col)\n",
    "\n",
    "    feature_cols.extend(new_cat_cols)\n",
    "    cat_cols = new_cat_cols\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "524daddf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T12:42:10.611611Z",
     "iopub.status.busy": "2025-01-26T12:42:10.611351Z",
     "iopub.status.idle": "2025-01-26T12:42:16.054913Z",
     "shell.execute_reply": "2025-01-26T12:42:16.053897Z"
    },
    "papermill": {
     "duration": 5.451013,
     "end_time": "2025-01-26T12:42:16.057243",
     "exception": false,
     "start_time": "2025-01-26T12:42:10.606230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = read_data('/kaggle/input/isic-2024-challenge/train-metadata.csv')\n",
    "df_test = read_data(\"/kaggle/input/isic-2024-challenge/test-metadata.csv\")\n",
    "\n",
    "df_train, df_test = preprocess(df_train, df_test)\n",
    "\n",
    "test_isic_ids = df_test.index.values\n",
    "\n",
    "x_test_tab = df_test[feature_cols]\n",
    "x_test_tab_xgb = df_test[feature_cols].values.copy()\n",
    "x_test_tab_xgb[x_test_tab_xgb == -np.inf] = 0\n",
    "x_test_tab_xgb[x_test_tab_xgb == np.inf] = 0\n",
    "x_test_tab_xgb = xgb.DMatrix(x_test_tab_xgb)\n",
    "\n",
    "if USE_IMAGE_FEAT:\n",
    "    image_pred_classes = [\n",
    "        'vit_tiny_384',\n",
    "        'swin_tiny_256',\n",
    "        'convnextv2_tiny_288',\n",
    "        'swin_tiny_224',\n",
    "        'convnextv2_base_128',\n",
    "        'convnextv2_large_64',\n",
    "        'coatnet_224',\n",
    "        'eb3_aux_224',\n",
    "        'mit_b5_aux_224',\n",
    "        'mit_b0_aux_384',\n",
    "    ]\n",
    "\n",
    "    meta = []\n",
    "    for isic_id, row in df_test.iterrows():\n",
    "        feat = img_feats_dict[isic_id]\n",
    "        meta.append(feat)\n",
    "    df_test[image_pred_classes] = np.array(meta, dtype=float)\n",
    "    del img_feats_dict\n",
    "\n",
    "    feature_cols_img_tab = image_pred_classes + feature_cols\n",
    "\n",
    "    x_test_tab_img = df_test[feature_cols_img_tab]\n",
    "    x_test_tab_img_xgb = df_test[feature_cols_img_tab].values.copy()\n",
    "    x_test_tab_img_xgb[x_test_tab_img_xgb == -np.inf] = 0\n",
    "    x_test_tab_img_xgb[x_test_tab_img_xgb == np.inf] = 0\n",
    "    x_test_tab_img_xgb = xgb.DMatrix(x_test_tab_img_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0863691",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T12:42:16.069393Z",
     "iopub.status.busy": "2025-01-26T12:42:16.068626Z",
     "iopub.status.idle": "2025-01-26T12:42:18.074685Z",
     "shell.execute_reply": "2025-01-26T12:42:18.073923Z"
    },
    "papermill": {
     "duration": 2.014275,
     "end_time": "2025-01-26T12:42:18.076899",
     "exception": false,
     "start_time": "2025-01-26T12:42:16.062624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tab_pred = []\n",
    "for fold in range(5):\n",
    "    lgb_ckpt_dir = '/kaggle/input/isic-2024-tab-ckpts/tab_only_v2_exp3/lgb/fold{}'.format(fold)\n",
    "    cb_ckpt_dir = '/kaggle/input/isic-2024-tab-ckpts/tab_only_v2_exp3/cb/fold{}'.format(fold)\n",
    "    xgb_ckpt_dir = '/kaggle/input/isic-2024-tab-ckpts/tab_only_v2_exp3/xgb/fold{}'.format(fold)\n",
    "    \n",
    "    for rdir, _, files in os.walk(lgb_ckpt_dir):\n",
    "        assert len(files) == 6\n",
    "        for file in files:\n",
    "            if '.txt' not in file:\n",
    "                continue\n",
    "            ckpt_path = os.path.join(rdir, file)\n",
    "            lgb_model = lgb.Booster(model_file=ckpt_path)\n",
    "            lgb_y_pred = lgb_model.predict(x_test_tab.values)\n",
    "            tab_pred.append(lgb_y_pred)\n",
    "    \n",
    "    for rdir, _, files in os.walk(cb_ckpt_dir):\n",
    "        assert len(files) == 6\n",
    "        for file in files:\n",
    "            if '.cbm' not in file:\n",
    "                continue\n",
    "            ckpt_path = os.path.join(rdir, file)\n",
    "            cb_model = cb.CatBoostClassifier()\n",
    "            cb_model.load_model(ckpt_path, format='cbm')\n",
    "            cb_y_pred = cb_model.predict_proba(x_test_tab)[:, 1]\n",
    "            tab_pred.append(cb_y_pred)\n",
    "            \n",
    "    for rdir, _, files in os.walk(xgb_ckpt_dir):\n",
    "        assert len(files) == 6\n",
    "        for file in files:\n",
    "            if '.json' not in file:\n",
    "                continue\n",
    "            ckpt_path = os.path.join(rdir, file)\n",
    "            xgb_model = xgb.Booster()\n",
    "            xgb_model.load_model(ckpt_path)\n",
    "            xgb_y_pred = xgb_model.predict(x_test_tab_xgb, iteration_range=(0, xgb_model.best_iteration+1))\n",
    "            tab_pred.append(xgb_y_pred)\n",
    "\n",
    "tab_pred = np.array(tab_pred)\n",
    "tab_pred = gmean(tab_pred, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59896eea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T12:42:18.091853Z",
     "iopub.status.busy": "2025-01-26T12:42:18.091361Z",
     "iopub.status.idle": "2025-01-26T12:42:19.930218Z",
     "shell.execute_reply": "2025-01-26T12:42:19.929478Z"
    },
    "papermill": {
     "duration": 1.848292,
     "end_time": "2025-01-26T12:42:19.932306",
     "exception": false,
     "start_time": "2025-01-26T12:42:18.084014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if USE_IMAGE_FEAT:\n",
    "    tab_img_pred = []\n",
    "    for fold in range(5):\n",
    "        lgb_ckpt_dir = '/kaggle/input/isic-2024-tab-ckpts/tab_img_10models_v2_exp3/lgb/fold{}'.format(fold)\n",
    "        cb_ckpt_dir = '/kaggle/input/isic-2024-tab-ckpts/tab_img_10models_v2_exp3/cb/fold{}'.format(fold)\n",
    "        xgb_ckpt_dir = '/kaggle/input/isic-2024-tab-ckpts/tab_img_10models_v2_exp3/xgb/fold{}'.format(fold)\n",
    "\n",
    "        for rdir, _, files in os.walk(lgb_ckpt_dir):\n",
    "            assert len(files) == 6\n",
    "            for file in files:\n",
    "                if '.txt' not in file:\n",
    "                    continue\n",
    "                ckpt_path = os.path.join(rdir, file)\n",
    "                lgb_model = lgb.Booster(model_file=ckpt_path)\n",
    "                lgb_y_pred = lgb_model.predict(x_test_tab_img.values)\n",
    "                tab_img_pred.append(lgb_y_pred)\n",
    "\n",
    "        for rdir, _, files in os.walk(cb_ckpt_dir):\n",
    "            assert len(files) == 6\n",
    "            for file in files:\n",
    "                if '.cbm' not in file:\n",
    "                    continue\n",
    "                ckpt_path = os.path.join(rdir, file)\n",
    "                cb_model = cb.CatBoostClassifier()\n",
    "                cb_model.load_model(ckpt_path, format='cbm')\n",
    "                cb_y_pred = cb_model.predict_proba(x_test_tab_img)[:, 1]\n",
    "                tab_img_pred.append(cb_y_pred)\n",
    "\n",
    "        for rdir, _, files in os.walk(xgb_ckpt_dir):\n",
    "            assert len(files) == 6\n",
    "            for file in files:\n",
    "                if '.json' not in file:\n",
    "                    continue\n",
    "                ckpt_path = os.path.join(rdir, file)\n",
    "                xgb_model = xgb.Booster()\n",
    "                xgb_model.load_model(ckpt_path)\n",
    "                xgb_y_pred = xgb_model.predict(x_test_tab_img_xgb, iteration_range=(0, xgb_model.best_iteration+1))\n",
    "                tab_img_pred.append(xgb_y_pred)\n",
    "    tab_img_pred = np.array(tab_img_pred)\n",
    "    tab_img_pred = gmean(tab_img_pred, 0)\n",
    "    \n",
    "    pred_ens = 0.2*tab_pred + 0.8*tab_img_pred\n",
    "    \n",
    "else:\n",
    "    pred_ens = tab_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e278fadb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T12:42:19.943525Z",
     "iopub.status.busy": "2025-01-26T12:42:19.943206Z",
     "iopub.status.idle": "2025-01-26T12:42:19.954201Z",
     "shell.execute_reply": "2025-01-26T12:42:19.953433Z"
    },
    "papermill": {
     "duration": 0.018496,
     "end_time": "2025-01-26T12:42:19.955946",
     "exception": false,
     "start_time": "2025-01-26T12:42:19.937450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame()\n",
    "df_sub['isic_id'] = test_isic_ids\n",
    "df_sub[\"target\"] = pred_ens\n",
    "df_sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33a4ab4b",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-26T12:42:19.970023Z",
     "iopub.status.busy": "2025-01-26T12:42:19.969780Z",
     "iopub.status.idle": "2025-01-26T12:42:19.982474Z",
     "shell.execute_reply": "2025-01-26T12:42:19.981688Z"
    },
    "papermill": {
     "duration": 0.021672,
     "end_time": "2025-01-26T12:42:19.984333",
     "exception": false,
     "start_time": "2025-01-26T12:42:19.962661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isic_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0015657</td>\n",
       "      <td>0.123083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0015729</td>\n",
       "      <td>0.056275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0015740</td>\n",
       "      <td>0.097015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        isic_id    target\n",
       "0  ISIC_0015657  0.123083\n",
       "1  ISIC_0015729  0.056275\n",
       "2  ISIC_0015740  0.097015"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3867fa",
   "metadata": {
    "papermill": {
     "duration": 0.006508,
     "end_time": "2025-01-26T12:42:19.997525",
     "exception": false,
     "start_time": "2025-01-26T12:42:19.991017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9094797,
     "sourceId": 63056,
     "sourceType": "competition"
    },
    {
     "datasetId": 5560703,
     "sourceId": 9197988,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5640336,
     "sourceId": 9318452,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5602809,
     "sourceId": 9318568,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 217.80627,
   "end_time": "2025-01-26T12:42:22.944123",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-26T12:38:45.137853",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
